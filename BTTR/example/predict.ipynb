{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bttr.lit_bttr import LitBTTR\n",
    "from bttr.datamodule import vocab\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '..\\\\checkpoints\\\\bttr_exemple\\\\bttr_exemple.ckpt'\n",
    "img_path = '33.png'\n",
    "#img_path = '18_em_1.bmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:201: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file c:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\checkpoints\\bttr_exemple\\bttr_exemple.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model = LitBTTR.load_from_checkpoint(ckpt, map_location=torch.device('cpu'))\n",
    "\n",
    "#Repare que esse trecho abaixo tem no notebook do CoMER, mas não no do BTTR. Verifica com calma a diferença de rodar com e sem.\n",
    "#model = model.eval()\n",
    "#device = torch.device(\"cpu\")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img = cv2.resize(img, (150,150), interpolation=cv2.INTER_AREA)\n",
    "img = Image.fromarray(img)\n",
    "# img = Image.open(img_path)\n",
    "# img = ImageOps.grayscale(img) #lembra de converter para cinza\n",
    "# img = ImageOps.pad(img,(150,150),method=Image.Resampling.BICUBIC) \n",
    "\n",
    "#Lembra de também converter para o tamanho que usamos no treinamento: (150,150)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=150x150>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ToTensor()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#beam_size é um parâmetro que variamos com optuna: pode ser 10 (default) ou 5\n",
    "beam_size = 10\n",
    "\n",
    "#max_len e alpha a gente não mexe: usamos esses valores fixos\n",
    "max_len = 200\n",
    "alpha = 1.0\n",
    "\n",
    "mask = torch.zeros_like(img, dtype=torch.bool)#repare aqui que eu troquei para bool (comparei com o do CoMER e com a funcao collate_fn em datamodule.py)\n",
    "hyps = model.bttr.beam_search(img.unsqueeze(0), mask, beam_size, max_len)\n",
    "best_hyp = max(hyps, key=lambda h: h.score / (len(h) ** alpha))\n",
    "hyp = vocab.indices2label(best_hyp.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "only bool and floating types of key_padding_mask are supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Isso aqui dá erro. A célula de cima é o ajuste para rodar\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hyp \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mbeam_search(img)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\BTTR\\bttr\\lit_bttr.py:96\u001b[0m, in \u001b[0;36mLitBTTR.beam_search\u001b[1;34m(self, img, beam_size, max_len, alpha)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39massert\u001b[39;00m img\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     95\u001b[0m img_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(img, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)  \u001b[39m# squeeze channel\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m hyps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbttr\u001b[39m.\u001b[39;49mbeam_search(img\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), img_mask, beam_size, max_len)\n\u001b[0;32m     97\u001b[0m best_hyp \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(hyps, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m h: h\u001b[39m.\u001b[39mscore \u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(h) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m alpha))\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m vocab\u001b[39m.\u001b[39mindices2label(best_hyp\u001b[39m.\u001b[39mseq)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\BTTR\\bttr\\model\\bttr.py:83\u001b[0m, in \u001b[0;36mBTTR.beam_search\u001b[1;34m(self, img, img_mask, beam_size, max_len)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"run bi-direction beam search for given img\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mList[Hypothesis]\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m feature, mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(img, img_mask)  \u001b[39m# [1, t, d]\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mbeam_search(feature, mask, beam_size, max_len)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\BTTR\\bttr\\model\\decoder.py:295\u001b[0m, in \u001b[0;36mDecoder.beam_search\u001b[1;34m(self, src, mask, beam_size, max_len)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbeam_search\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m, src: FloatTensor, mask: LongTensor, beam_size: \u001b[39mint\u001b[39m, max_len: \u001b[39mint\u001b[39m\n\u001b[0;32m    279\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Hypothesis]:\n\u001b[0;32m    280\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"run beam search for src img\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39m    List[Hypothesis]\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     l2r_hypos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_beam_search(src, mask, \u001b[39m\"\u001b[39;49m\u001b[39ml2r\u001b[39;49m\u001b[39m\"\u001b[39;49m, beam_size, max_len)\n\u001b[0;32m    296\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross_rate_score(src, mask, l2r_hypos, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr2l\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    298\u001b[0m     r2l_hypos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_beam_search(src, mask, \u001b[39m\"\u001b[39m\u001b[39mr2l\u001b[39m\u001b[39m\"\u001b[39m, beam_size, max_len)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\BTTR\\bttr\\model\\decoder.py:180\u001b[0m, in \u001b[0;36mDecoder._beam_search\u001b[1;34m(self, src, mask, direction, beam_size, max_len)\u001b[0m\n\u001b[0;32m    177\u001b[0m exp_src \u001b[39m=\u001b[39m repeat(src\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39ms e -> b s e\u001b[39m\u001b[39m\"\u001b[39m, b\u001b[39m=\u001b[39mhyp_num)\n\u001b[0;32m    178\u001b[0m exp_mask \u001b[39m=\u001b[39m repeat(mask\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39ms -> b s\u001b[39m\u001b[39m\"\u001b[39m, b\u001b[39m=\u001b[39mhyp_num)\n\u001b[1;32m--> 180\u001b[0m decode_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(exp_src, exp_mask, hypotheses)[:, t, :]\n\u001b[0;32m    181\u001b[0m log_p_t \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(decode_outputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    183\u001b[0m live_hyp_num \u001b[39m=\u001b[39m beam_size \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(completed_hypotheses)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\01011000\\Documents\\GitHub\\Train_and_infer\\BTTR\\bttr\\model\\decoder.py:111\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, src, src_mask, tgt)\u001b[0m\n\u001b[0;32m    108\u001b[0m src \u001b[39m=\u001b[39m rearrange(src, \u001b[39m\"\u001b[39m\u001b[39mb t d -> t b d\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m tgt \u001b[39m=\u001b[39m rearrange(tgt, \u001b[39m\"\u001b[39m\u001b[39mb l d -> l b d\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m    112\u001b[0m     tgt\u001b[39m=\u001b[39;49mtgt,\n\u001b[0;32m    113\u001b[0m     memory\u001b[39m=\u001b[39;49msrc,\n\u001b[0;32m    114\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    115\u001b[0m     tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_pad_mask,\n\u001b[0;32m    116\u001b[0m     memory_key_padding_mask\u001b[39m=\u001b[39;49msrc_mask,\n\u001b[0;32m    117\u001b[0m )\n\u001b[0;32m    119\u001b[0m out \u001b[39m=\u001b[39m rearrange(out, \u001b[39m\"\u001b[39m\u001b[39ml b d -> b l d\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:360\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    357\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    359\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 360\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    361\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    362\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    363\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:699\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m--> 699\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    700\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    702\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:717\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[1;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[0;32m    716\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 717\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[0;32m    718\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    719\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    720\u001b[0m                             is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[0;32m    721\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    722\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1083\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1079\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly allow causal mask or attn_mask\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1081\u001b[0m is_batched \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m-> 1083\u001b[0m key_padding_mask \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49m_canonical_mask(\n\u001b[0;32m   1084\u001b[0m     mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m   1085\u001b[0m     mask_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkey_padding_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1086\u001b[0m     other_type\u001b[39m=\u001b[39;49mF\u001b[39m.\u001b[39;49m_none_or_dtype(attn_mask),\n\u001b[0;32m   1087\u001b[0m     other_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mattn_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1088\u001b[0m     target_type\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mdtype\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1091\u001b[0m why_not_fast_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n",
      "File \u001b[1;32mc:\\Users\\01011000\\AppData\\Local\\anaconda3\\envs\\SAN\\lib\\site-packages\\torch\\nn\\functional.py:4995\u001b[0m, in \u001b[0;36m_canonical_mask\u001b[1;34m(mask, mask_name, other_type, other_name, target_type, check_other)\u001b[0m\n\u001b[0;32m   4993\u001b[0m _mask_is_float \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_floating_point(mask)\n\u001b[0;32m   4994\u001b[0m \u001b[39mif\u001b[39;00m _mask_dtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mbool \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _mask_is_float:\n\u001b[1;32m-> 4995\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   4996\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monly bool and floating types of \u001b[39m\u001b[39m{\u001b[39;00mmask_name\u001b[39m}\u001b[39;00m\u001b[39m are supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4997\u001b[0m \u001b[39mif\u001b[39;00m check_other \u001b[39mand\u001b[39;00m other_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4998\u001b[0m     \u001b[39mif\u001b[39;00m _mask_dtype \u001b[39m!=\u001b[39m other_type:\n",
      "\u001b[1;31mAssertionError\u001b[0m: only bool and floating types of key_padding_mask are supported"
     ]
    }
   ],
   "source": [
    "#Isso aqui dá erro. A célula de cima é o ajuste para rodar\n",
    "hyp = model.beam_search(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\overset { 1 } { 1 } 8 + 1 5 = 3 3\n"
     ]
    }
   ],
   "source": [
    "print(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bttr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c10e69c8be8251d2427514ec023c1218910a696d384ef8f31a18bcc6602a077"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
